{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Author Hsieh Lee Hsun\n",
    "# @Date 2018/11/16\n",
    "# @Description: Weather data from EPA data \n",
    "import pickle as pk\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from random import randint\n",
    "import json\n",
    "import math\n",
    "import geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from pk which gets from mongodb\n",
    "data = list()\n",
    "for i in range(4, 6, 1):\n",
    "    with open(\"All_PM25_data_201\" + str(i) + \".pk\",\"rb\") as file:\n",
    "        data.append(pk.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data from 2014 ~ 2016\n",
    "#Construct a dictionary contains datas of every hours\n",
    "overall_data = list()\n",
    "#for feature in feature_set:\n",
    "#    overall_data[feature] = list()\n",
    "for i in range(2):\n",
    "    for index in tqdm(range(len(data[i]))):\n",
    "        d = data[i][index]\n",
    "        is_pm25 = False\n",
    "        if d['feature'] == \"PM2.5\":\n",
    "            is_pm25 = True\n",
    "        for hour in d['hr_data']:\n",
    "            individual_data = dict()\n",
    "            if hour['hr_flag']:\n",
    "                individual_data['data'] = 0\n",
    "            else:\n",
    "                individual_data['data'] = hour['hr_v']\n",
    "            individual_data['date'] = datetime(d['date']['year'], d['date']['month'], \n",
    "                                               d['date']['day'], hour['hr_t'])\n",
    "            individual_data['station'] = d['station']\n",
    "            individual_data['feature'] = d['feature']\n",
    "            overall_data.append(individual_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump all the data to the pickle\n",
    "# Release the memory space\n",
    "del data\n",
    "with open('All_PM25_data_DataFrame_list_2014_to_2015.pk', 'wb') as file:\n",
    "    pk.dump(overall_data, file)\n",
    "del overall_data[:]\n",
    "del overall_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from pk which gets from mongodb\n",
    "with open(\"All_PM25_data_2017_8_9.pk\",\"rb\") as file:\n",
    "    data = pk.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data from 2014 ~ 2016\n",
    "#Construct a dictionary contains datas of every hours\n",
    "overall_data = list()\n",
    "#for feature in feature_set:\n",
    "#    overall_data[feature] = list()\n",
    "for index in tqdm(range(len(data))):\n",
    "    d = data[index]\n",
    "    is_pm25 = False\n",
    "    if d['feature'] == \"PM2.5\":\n",
    "        is_pm25 = True\n",
    "    for hour in d['hr_data']:\n",
    "        individual_data = dict()\n",
    "        if hour['hr_flag']:\n",
    "            individual_data['data'] = 0\n",
    "        else:\n",
    "            individual_data['data'] = hour['hr_v']\n",
    "        individual_data['date'] = datetime(d['date']['year'], d['date']['month'],\n",
    "                                           d['date']['day'], hour['hr_t'])\n",
    "        individual_data['station'] = d['station']\n",
    "        individual_data['feature'] = d['feature']\n",
    "        overall_data.append(individual_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump all the data to the pickle\n",
    "# Release the memory space\n",
    "del data\n",
    "with open('All_PM25_data_DataFrame_list_2017_8_9.pk', 'wb') as file:\n",
    "    pk.dump(overall_data, file)\n",
    "del overall_data[:]\n",
    "del overall_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('All_PM25_data_DataFrame_list_2014_to_2015.pk', 'rb') as file:\n",
    "    overall_data = pk.load(file)\n",
    "with open('All_PM25_data_DataFrame_list_2016_to_2017.pk', 'rb') as file:\n",
    "    overall_data_2 = pk.load(file)\n",
    "with open('All_PM25_data_DataFrame_list_2017_8_9.pk', 'rb') as file:\n",
    "    overall_data_3 = pk.load(file)\n",
    "overall_data += overall_data_2\n",
    "overall_data += overall_data_3\n",
    "del overall_data_2[:]\n",
    "del overall_data_2\n",
    "del overall_data_3[:]\n",
    "del overall_data_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape all the data and use 'date' as row index and 'station' as column\n",
    "date = pd.date_range(start='1/1/2014', end='10/1/2017', freq='H', closed='left')\n",
    "overall_dataframe = pd.DataFrame(overall_data).pivot_table(index='date', \n",
    "                                                           columns=['station', 'feature'], \n",
    "                                                           values='data', \n",
    "                                                           aggfunc='first').reindex(date)\n",
    "# Reference: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html\n",
    "# Serialize the result\n",
    "with open(\"All_PM25_data_DataFrame_training.pk\",\"wb\") as file:\n",
    "    pk.dump(overall_dataframe, file)\n",
    "# End of All_PM25_data_DataFrame.pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the 2014 ~ 2017 training data including normalize\n",
    "#Read the data from pickle\n",
    "with open(\"All_PM25_data_DataFrame_training.pk\",\"rb\") as file:\n",
    "    test = pk.load(file)\n",
    "\n",
    "interpolate_idx = pd.Index(['AMB_TEMP', 'PM10', 'PM2.5', 'RH', 'WIND_SPEED', 'WS_HR'])\n",
    "\n",
    "def f(x):\n",
    "    return (x.apply(lambda x : x if x > 0 else 0))\n",
    "\n",
    "test = test.drop('馬祖東引', axis=1)\n",
    "test = test.drop([(\"陽明\", \"WIND_DIREC\"), (\"陽明\", \"WIND_SPEED\"), (\"淡水\", \"WIND_DIREC\"), (\"淡水\", \"WIND_SPEED\")], axis=1)\n",
    "test = test.apply(f)\n",
    "test = test.replace('null', np.nan)\n",
    "test.loc[:, (slice(None), interpolate_idx)] = \\\n",
    "    test.loc[:, (slice(None), interpolate_idx)].replace(0, np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['三重', '大同', '淡水', '陽明'], dtype='object', name='station')\n",
      "feature              AMB_TEMP  PM10  PM2.5  RH\n",
      "2014-01-01 00:00:00       NaN  46.0   26.0 NaN\n",
      "2014-01-01 01:00:00       NaN  45.0   28.0 NaN\n",
      "2014-01-01 02:00:00       NaN  50.0   31.0 NaN\n",
      "2014-01-01 03:00:00       NaN  60.0   35.0 NaN\n",
      "2014-01-01 04:00:00       NaN  57.0   36.0 NaN\n",
      "feature              AMB_TEMP  PM10  PM2.5    RH  WD_HR  WIND_DIREC  \\\n",
      "2014-01-01 00:00:00      17.0  53.0   36.0  87.0  251.0       244.0   \n",
      "2014-01-01 01:00:00      17.0  51.0   34.0  87.0  266.0       267.0   \n",
      "2014-01-01 02:00:00      17.0  42.0   27.0  86.0  249.0       240.0   \n",
      "2014-01-01 03:00:00      17.0  37.0   23.0  85.0  255.0       272.0   \n",
      "2014-01-01 04:00:00      17.0  32.0   21.0  84.0  239.0       237.0   \n",
      "\n",
      "feature              WIND_SPEED  WS_HR  \n",
      "2014-01-01 00:00:00         1.9    1.6  \n",
      "2014-01-01 01:00:00         1.5    1.3  \n",
      "2014-01-01 02:00:00         1.6    1.2  \n",
      "2014-01-01 03:00:00         1.2    1.2  \n",
      "2014-01-01 04:00:00         2.2    1.6  \n"
     ]
    }
   ],
   "source": [
    "# Choose the valid row of data\n",
    "valid_col = pd.Index(['AMB_TEMP', 'PM10', 'PM2.5', 'RH', 'WIND_DIREC', 'WIND_SPEED', 'WD_HR', 'WS_HR'])\n",
    "valid_test_set = test.loc[:, (slice(None), valid_col)]\n",
    "\n",
    "# Findout sites without wind direction but PM2.5\n",
    "idx1 = valid_test_set.loc[:, (slice(None), 'WIND_DIREC')].columns.get_level_values(0)\n",
    "idx2 = valid_test_set.loc[:, (slice(None), 'PM2.5')].columns.get_level_values(0)\n",
    "without_direc = idx2.difference(idx1)\n",
    "print(without_direc)\n",
    "\n",
    "# Print two examples of data\n",
    "print(valid_test_set.loc[:, '淡水'].head())\n",
    "print(valid_test_set.loc[:, '花蓮'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['三義', '中壢', '中山', '二林', '仁武', '冬山', '前金', '前鎮', '南投', '古亭', '善化', '嘉義',\n",
       "       '土城', '埔里', '基隆', '士林', '大園', '大寮', '大里', '安南', '宜蘭', '小港', '屏東', '崙背',\n",
       "       '左營', '平鎮', '彰化', '復興', '忠明', '恆春', '斗六', '新店', '新港', '新營', '新竹', '新莊',\n",
       "       '朴子', '松山', '板橋', '林口', '林園', '桃園', '楠梓', '橋頭', '永和', '汐止', '沙鹿', '淡水',\n",
       "       '湖口', '潮州', '竹山', '竹東', '線西', '美濃', '臺南', '臺東', '臺西', '花蓮', '苗栗', '菜寮',\n",
       "       '萬華', '萬里', '西屯', '觀音', '豐原', '金門', '關山', '陽明', '頭份', '馬公', '馬祖', '鳳山',\n",
       "       '麥寮', '龍潭'],\n",
       "      dtype='object', name='station')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_test_set.loc[:, (slice(None), 'WIND_DIREC')].columns.get_level_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/.local/lib/python3.5/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Construct the cos and sine component of Direction and the PM2.5_diff and Normalize as well\n",
    "\n",
    "The base of the data\n",
    "\n",
    "PM2.5 500\n",
    "PM10 500\n",
    "WS_HR 30\n",
    "WD_HR(cos, sin) 1\n",
    "WIND_SPEED 30\n",
    "WIND_DIREC(cos, sin) 1\n",
    "AMB_TEMP 50\n",
    "RH 100\n",
    "\n",
    "'''\n",
    "\n",
    "valid_test_set.loc[:, (slice(None), 'PM2.5')] = valid_test_set.loc[:, (slice(None), 'PM2.5')] / 500\n",
    "valid_test_set.loc[:, (slice(None), 'PM10')] = valid_test_set.loc[:, (slice(None), 'PM10')] / 500\n",
    "valid_test_set.loc[:, (slice(None), 'WS_HR')] = valid_test_set.loc[:, (slice(None), 'WS_HR')] / 30\n",
    "valid_test_set.loc[:, (slice(None), 'WIND_SPEED')] = valid_test_set.loc[:, (slice(None), 'WIND_SPEED')] / 30\n",
    "valid_test_set.loc[:, (slice(None), 'AMB_TEMP')] = valid_test_set.loc[:, (slice(None), 'AMB_TEMP')] / 50\n",
    "valid_test_set.loc[:, (slice(None), 'RH')] = valid_test_set.loc[:, (slice(None), 'RH')] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/.local/lib/python3.5/site-packages/pandas/core/indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/home/sean/.local/lib/python3.5/site-packages/pandas/core/indexing.py:630: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "without_wind = False\n",
    "\n",
    "for station in set(valid_test_set.columns.get_level_values(0)):\n",
    "    \n",
    "    without_wind = (without_direc == station).any()\n",
    "    \n",
    "    # Fill the missing data\n",
    "    if without_wind:\n",
    "        valid_test_set.loc[:, (station, 'WIND_DIREC_COS')] = 0\n",
    "        valid_test_set.loc[:, (station, 'WIND_DIREC_SIN')] = 0\n",
    "        valid_test_set.loc[:, (station, 'WD_HR_COS')] = 0\n",
    "        valid_test_set.loc[:, (station, 'WD_HR_SIN')] = 0\n",
    "        valid_test_set.loc[:, (station, 'WIND_SPEED')] = 0\n",
    "        valid_test_set.loc[:, (station, 'WS_HR')] = 0\n",
    "        if station == '淡水' or station == '大同':\n",
    "            valid_test_set.loc[:, (station, 'AMB_TEMP')] = 0\n",
    "            valid_test_set.loc[:, (station, 'RH')] = 0\n",
    "        \n",
    "    else:\n",
    "        try:\n",
    "            valid_test_set.loc[:, (station, 'WIND_DIREC_COS')] = valid_test_set.loc[:, (station, 'WIND_DIREC')].apply(lambda x : x / 180 * np.pi).apply(np.cos)\n",
    "            valid_test_set.loc[:, (station, 'WIND_DIREC_SIN')] = valid_test_set.loc[:, (station, 'WIND_DIREC')].apply(lambda x : x / 180 * np.pi).apply(np.sin)        \n",
    "            valid_test_set.loc[:, (station, 'WD_HR_COS')] = valid_test_set.loc[:, (station, 'WD_HR')].apply(lambda x : x / 180 * np.pi).apply(np.cos)\n",
    "            valid_test_set.loc[:, (station, 'WD_HR_SIN')] = valid_test_set.loc[:, (station, 'WD_HR')].apply(lambda x : x / 180 * np.pi).apply(np.sin)\n",
    "        except:\n",
    "            print(station, \" Failed!\")\n",
    "        \n",
    "    valid_test_set.loc[:, (station, 'PM2.5_diff')] = valid_test_set.loc[:, (station, 'PM2.5')].diff()\n",
    "\n",
    "# Drop the Wind direction\n",
    "valid_test_set = valid_test_set.drop('WIND_DIREC', axis = 1, level = 1)\n",
    "valid_test_set = valid_test_set.drop('WD_HR', axis = 1, level = 1)\n",
    "valid_test_set = valid_test_set.interpolate(limit_direction = \"both\")\n",
    "valid_test_set = valid_test_set.sort_index(1)\n",
    "# valid_test_set.head()\n",
    "print(valid_test_set.isna().any().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>feature</th>\n",
       "      <th>AMB_TEMP</th>\n",
       "      <th>PM10</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM2.5_diff</th>\n",
       "      <th>RH</th>\n",
       "      <th>WD_HR_COS</th>\n",
       "      <th>WD_HR_SIN</th>\n",
       "      <th>WIND_DIREC_COS</th>\n",
       "      <th>WIND_DIREC_SIN</th>\n",
       "      <th>WIND_SPEED</th>\n",
       "      <th>WS_HR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-09-30 19:00:00</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.438371</td>\n",
       "      <td>-0.898794</td>\n",
       "      <td>0.669131</td>\n",
       "      <td>-0.743145</td>\n",
       "      <td>0.036667</td>\n",
       "      <td>0.036667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30 20:00:00</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.882948</td>\n",
       "      <td>-0.469472</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>0.040132</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.023333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30 21:00:00</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.224951</td>\n",
       "      <td>-0.974370</td>\n",
       "      <td>0.173648</td>\n",
       "      <td>-0.984808</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.036667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30 22:00:00</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.406737</td>\n",
       "      <td>-0.913545</td>\n",
       "      <td>-0.017452</td>\n",
       "      <td>-0.999848</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.026667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30 23:00:00</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.848048</td>\n",
       "      <td>0.529919</td>\n",
       "      <td>0.891007</td>\n",
       "      <td>0.453990</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.013333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "feature              AMB_TEMP   PM10  PM2.5  PM2.5_diff    RH  WD_HR_COS  \\\n",
       "2017-09-30 19:00:00       0.6  0.074  0.032      -0.008  0.75   0.438371   \n",
       "2017-09-30 20:00:00       0.6  0.072  0.034       0.002  0.77   0.882948   \n",
       "2017-09-30 21:00:00       0.6  0.076  0.036       0.002  0.78   0.224951   \n",
       "2017-09-30 22:00:00       0.6  0.066  0.038       0.002  0.79   0.406737   \n",
       "2017-09-30 23:00:00       0.6  0.066  0.044       0.006  0.80   0.848048   \n",
       "\n",
       "feature              WD_HR_SIN  WIND_DIREC_COS  WIND_DIREC_SIN  WIND_SPEED  \\\n",
       "2017-09-30 19:00:00  -0.898794        0.669131       -0.743145    0.036667   \n",
       "2017-09-30 20:00:00  -0.469472        0.999194        0.040132    0.033333   \n",
       "2017-09-30 21:00:00  -0.974370        0.173648       -0.984808    0.053333   \n",
       "2017-09-30 22:00:00  -0.913545       -0.017452       -0.999848    0.033333   \n",
       "2017-09-30 23:00:00   0.529919        0.891007        0.453990    0.026667   \n",
       "\n",
       "feature                 WS_HR  \n",
       "2017-09-30 19:00:00  0.036667  \n",
       "2017-09-30 20:00:00  0.023333  \n",
       "2017-09-30 21:00:00  0.036667  \n",
       "2017-09-30 22:00:00  0.026667  \n",
       "2017-09-30 23:00:00  0.013333  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_test_set.loc[:, '臺南'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./knn_dtw.json', 'r') as f:\n",
    "    knn_dtw_data = json.load(f)\n",
    "with open('./knn_ed.json', 'r') as f:\n",
    "    knn_ed_data = json.load(f)\n",
    "with open('./station.pk', 'rb') as f:\n",
    "    station = pk.load(f)\n",
    "# Form two dictionaries to project from id to name and name from id\n",
    "\n",
    "station_to_num = dict()\n",
    "num_to_station = dict()\n",
    "for data in station:\n",
    "    station_to_num[data['Sitename']] = str(data['id'])\n",
    "    num_to_station[str(data['id'])] = data['Sitename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Straight store individual station data\n",
    "\n",
    "station_list = list()\n",
    "\n",
    "training_date = pd.date_range(start='1/1/2014', end='10/1/2016', freq='H', closed='left')\n",
    "val_date = pd.date_range(start='10/1/2016', end='10/1/2017', freq='H', closed='left')\n",
    "\n",
    "for station in set(valid_test_set.columns.get_level_values(0)):\n",
    "    with open(\"./Seq2Seq_history_df/All_PM25_DataFrame_training_\" + station_to_num[station] + \".pk\", \"wb\") as file:\n",
    "        pk.dump(valid_test_set.loc[training_date, station], file)\n",
    "    with open(\"./Seq2Seq_history_df/All_PM25_DataFrame_label_\" + station_to_num[station] + \".pk\", \"wb\") as file:\n",
    "        pk.dump(valid_test_set.loc[training_date, (station, 'PM2.5')], file)\n",
    "    with open(\"./Seq2Seq_history_df/All_PM25_DataFrame_val_\" + station_to_num[station] + \".pk\", \"wb\") as file:\n",
    "        pk.dump(valid_test_set.loc[val_date, station], file)\n",
    "    with open(\"./Seq2Seq_history_df/All_PM25_DataFrame_val_label_\" + station_to_num[station] + \".pk\", \"wb\") as file:\n",
    "        pk.dump(valid_test_set.loc[val_date, (station, 'PM2.5')], file)\n",
    "    \n",
    "    station_list.append(station_to_num[station])\n",
    "        \n",
    "with open(\"./Seq2Seq_history_df/station_list.pk\", \"wb\") as file:\n",
    "    pk.dump(station_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "橋頭  Ok !\n",
      "中山  Ok !\n",
      "恆春  Ok !\n",
      "平鎮  Ok !\n",
      "楠梓  Ok !\n",
      "淡水  Ok !\n",
      "臺南  Ok !\n",
      "菜寮  Ok !\n",
      "馬公  Ok !\n",
      "士林  Ok !\n",
      "麥寮  Ok !\n",
      "新店  Ok !\n",
      "基隆  Ok !\n",
      "嘉義  Ok !\n",
      "頭份  Ok !\n",
      "沙鹿  Ok !\n",
      "西屯  Ok !\n",
      "苗栗  Ok !\n",
      "林園  Ok !\n",
      "陽明  Ok !\n",
      "馬祖  Ok !\n",
      "萬里  Ok !\n",
      "安南  Ok !\n",
      "潮州  Ok !\n",
      "龍潭  Ok !\n",
      "萬華  Ok !\n",
      "冬山  Ok !\n",
      "大寮  Ok !\n",
      "線西  Ok !\n",
      "觀音  Ok !\n",
      "大同  Ok !\n",
      "新莊  Ok !\n",
      "古亭  Ok !\n",
      "桃園  Ok !\n",
      "美濃  Ok !\n",
      "臺東  Ok !\n",
      "埔里  Ok !\n",
      "新港  Ok !\n",
      "朴子  Ok !\n",
      "林口  Ok !\n",
      "花蓮  Ok !\n",
      "關山  Ok !\n",
      "二林  Ok !\n",
      "鳳山  Ok !\n",
      "金門  Ok !\n",
      "中壢  Ok !\n",
      "崙背  Ok !\n",
      "小港  Ok !\n",
      "三義  Ok !\n",
      "屏東  Ok !\n",
      "永和  Ok !\n",
      "南投  Ok !\n",
      "豐原  Ok !\n",
      "新竹  Ok !\n",
      "前金  Ok !\n",
      "復興  Ok !\n",
      "竹山  Ok !\n",
      "汐止  Ok !\n",
      "斗六  Ok !\n",
      "新營  Ok !\n",
      "湖口  Ok !\n",
      "仁武  Ok !\n",
      "大園  Ok !\n",
      "宜蘭  Ok !\n",
      "左營  Ok !\n",
      "三重  Ok !\n",
      "松山  Ok !\n",
      "土城  Ok !\n",
      "前鎮  Ok !\n",
      "彰化  Ok !\n",
      "大里  Ok !\n",
      "臺西  Ok !\n",
      "板橋  Ok !\n",
      "忠明  Ok !\n",
      "善化  Ok !\n",
      "竹東  Ok !\n",
      "station num 76\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data into (Batch_size, 77, 6) according to the result of dtw and ed\n",
    "\n",
    "station_num = 0\n",
    "avaliable_station = list()\n",
    "\n",
    "overall_ed_dtw_station = list()\n",
    "\n",
    "training_date = pd.date_range(start='1/1/2014', end='10/1/2016', freq='H', closed='left')\n",
    "val_date = pd.date_range(start='10/1/2016', end='10/1/2017', freq='H', closed='left')\n",
    "\n",
    "for station in set(valid_test_set.columns.get_level_values(0)):\n",
    "    \n",
    "    # Read the ed and dtw into a list forming a index\n",
    "    ed_station_list = list()\n",
    "    ed_dtw_station = list()\n",
    "    # The current station itself is in the list of ed\n",
    "    ed_station_list.append(station)\n",
    "    # Overall ed dtw station\n",
    "    ed_dtw_station.append(station_to_num[station])\n",
    "    \n",
    "    dtw_station_list = list()\n",
    "    \n",
    "    ed = knn_ed_data[station_to_num[station]]\n",
    "    dtw = knn_dtw_data[station_to_num[station]]\n",
    "\n",
    "    for i in range(3):\n",
    "        ed_station_list.append(num_to_station[ed[i]])\n",
    "        dtw_station_list.append(num_to_station[dtw[i]])\n",
    "        ed_dtw_station.append(ed[i])\n",
    "        ed_dtw_station.append(dtw[i])\n",
    "    \n",
    "    ed_idx = pd.Index(ed_station_list, name='station')\n",
    "    dtw_idx = pd.Index(dtw_station_list, name='station')\n",
    "    \n",
    "#     if station_to_num[station] == \"4\":\n",
    "#         print(dtw_idx)\n",
    "#         print(valid_test_set.loc[:, (dtw_idx, feature_index)])\n",
    "    \n",
    "    # Get those station\n",
    "    test_set_npy = np.concatenate((valid_test_set.loc[training_date, ed_idx].values, \n",
    "                                   valid_test_set.loc[training_date, dtw_idx].values), axis = 1)\n",
    "    \n",
    "    pm25 = valid_test_set.loc[training_date, (station, 'PM2.5')].values\n",
    "    \n",
    "    val_set_npy = np.concatenate((valid_test_set.loc[val_date, ed_idx].values, \n",
    "                                  valid_test_set.loc[val_date, dtw_idx].values), axis = 1)\n",
    "    \n",
    "    val_pm25 = valid_test_set.loc[val_date, (station, 'PM2.5')].values\n",
    "    \n",
    "    # If the shape doesn't match\n",
    "    if(test_set_npy.shape[1] != 77):\n",
    "        print(test_set_npy.shape)\n",
    "        print(ed_idx)\n",
    "        print(dtw_idx)\n",
    "        print(station, \" failed !\")\n",
    "        continue\n",
    "        \n",
    "    #Prepare two empty numpy arrays\n",
    "    training_set = np.empty([test_set_npy.shape[0] - 12, 77, 6], dtype=np.float32)\n",
    "    label_set = np.empty([test_set_npy.shape[0] - 12, 6], dtype=np.float32)\n",
    "    \n",
    "    val_set = np.empty([val_set_npy.shape[0] - 12, 77, 6], dtype=np.float32)\n",
    "    val_label_set = np.empty([val_set_npy.shape[0] - 12, 6], dtype=np.float32)\n",
    "    \n",
    "    for i in range(test_set_npy.shape[0] - 12):\n",
    "        training_set[i] = test_set_npy[i : i + 6].T\n",
    "        label_set[i] = pm25[i + 6 : i + 12]\n",
    "    \n",
    "    for i in range(val_set_npy.shape[0] - 12):\n",
    "        val_set[i] = val_set_npy[i : i + 6].T\n",
    "        val_label_set[i] = val_pm25[i + 6 : i + 12]\n",
    "    \n",
    "    # Save those data\n",
    "    np.save('history_npy/{}.npy'.format(station_to_num[station]), training_set)\n",
    "    np.save('history_npy/{}_label.npy'.format(station_to_num[station]), label_set)\n",
    "    \n",
    "    # Save those data\n",
    "    np.save('history_npy/{}_val.npy'.format(station_to_num[station]), val_set)\n",
    "    np.save('history_npy/{}_val_label.npy'.format(station_to_num[station]), val_label_set)\n",
    "    \n",
    "    # Calculate the number of stations\n",
    "    station_num += 1\n",
    "    avaliable_station.append(station_to_num[station])\n",
    "    \n",
    "    # Overall station\n",
    "    overall_ed_dtw_station.append(ed_dtw_station)\n",
    "    \n",
    "    print(station, \" Ok !\")\n",
    "\n",
    "# Overall number of station\n",
    "print(\"station num\", station_num)\n",
    "\n",
    "with open('./history_npy/station_list.pk', 'wb') as file:\n",
    "    pk.dump(avaliable_station, file)\n",
    "    \n",
    "with open(\"overall_ed_dtw_station.pk\", \"wb\") as file:\n",
    "    pk.dump(overall_ed_dtw_station, file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature              AMB_TEMP   PM10  PM2.5  PM2.5_diff  RH  WD_HR_COS  \\\n",
      "2014-01-01 00:00:00         0  0.198  0.080      -0.014   0          0   \n",
      "2014-01-01 01:00:00         0  0.188  0.066      -0.014   0          0   \n",
      "2014-01-01 02:00:00         0  0.180  0.068       0.002   0          0   \n",
      "2014-01-01 03:00:00         0  0.172  0.044      -0.024   0          0   \n",
      "2014-01-01 04:00:00         0  0.156  0.052       0.008   0          0   \n",
      "\n",
      "feature              WD_HR_SIN  WIND_DIREC_COS  WIND_DIREC_SIN  WIND_SPEED  \\\n",
      "2014-01-01 00:00:00          0               0               0           0   \n",
      "2014-01-01 01:00:00          0               0               0           0   \n",
      "2014-01-01 02:00:00          0               0               0           0   \n",
      "2014-01-01 03:00:00          0               0               0           0   \n",
      "2014-01-01 04:00:00          0               0               0           0   \n",
      "\n",
      "feature              WS_HR  \n",
      "2014-01-01 00:00:00      0  \n",
      "2014-01-01 01:00:00      0  \n",
      "2014-01-01 02:00:00      0  \n",
      "2014-01-01 03:00:00      0  \n",
      "2014-01-01 04:00:00      0  \n",
      "[[ 0.     0.198  0.08  -0.014  0.     0.     0.     0.     0.     0.\n",
      "   0.   ]\n",
      " [ 0.     0.188  0.066 -0.014  0.     0.     0.     0.     0.     0.\n",
      "   0.   ]\n",
      " [ 0.     0.18   0.068  0.002  0.     0.     0.     0.     0.     0.\n",
      "   0.   ]\n",
      " [ 0.     0.172  0.044 -0.024  0.     0.     0.     0.     0.     0.\n",
      "   0.   ]\n",
      " [ 0.     0.156  0.052  0.008  0.     0.     0.     0.     0.     0.\n",
      "   0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print(valid_test_set.loc[:, \"大同\"].head())\n",
    "print(valid_test_set.loc[:, \"大同\"].head().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Return the lat, lon and height    \n",
    "    \n",
    "def get_height(st_no):\n",
    "    with open('height121.json') as df:\n",
    "        data = json.loads(df.read())\n",
    "    raw_temp = []\n",
    "    for i in range(len(data)):\n",
    "        raw_elem = []\n",
    "        if(data[i]['id'] == st_no):\n",
    "            raw_elem.append(data[i]['y'])\n",
    "            raw_elem.append(data[i]['x'])\n",
    "            raw_elem.append(data[i]['elevation'])\n",
    "            raw_elem.append(data[i]['latitude'])\n",
    "            raw_elem.append(data[i]['longitude'])\n",
    "            raw_temp.append(raw_elem)\n",
    "    \n",
    "\n",
    "    \n",
    "    h_matr_l = int(math.sqrt(len(raw_temp)))\n",
    "    h_lat_lon = np.empty((h_matr_l, h_matr_l, 2))\n",
    "    h_matr = np.empty((h_matr_l, h_matr_l))\n",
    "    \n",
    "    \n",
    "    for elem in raw_temp:\n",
    "        h_matr[elem[0] - 1][elem[1] - 1] = elem[2]\n",
    "        h_lat_lon[elem[0] - 1][elem[1] - 1][0] = elem[3]\n",
    "        h_lat_lon[elem[0] - 1][elem[1] - 1][1] = elem[4]\n",
    "        \n",
    "    return h_matr / 1000, h_lat_lon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all the distance of the map of cnn around all the sites\n",
    "\n",
    "from geopy import distance\n",
    "\n",
    "with open('station.pk', 'rb') as file:\n",
    "    station = pk.load(file)\n",
    "    \n",
    "station_dict = dict()\n",
    "\n",
    "for s in station:\n",
    "    station_dict[s['id'] - 1] = (s['lat'], s['lng'])\n",
    "\n",
    "lat_lon_dict = dict()\n",
    "height_dict = dict()\n",
    "idw_denominator = dict()\n",
    "\n",
    "for site in range(76):\n",
    "    #print(site)\n",
    "    h_matr, h_lat_lon = get_height(site + 1)\n",
    "    d = np.zeros((11, 11, 76))\n",
    "    for i in range(11):\n",
    "        for j in range(11):\n",
    "            for s in station_dict:\n",
    "                d[i, j, s] = distance.distance(station_dict[s], h_lat_lon[i][j]).km\n",
    "                \n",
    "    lat_lon_dict[str(site)] = d\n",
    "    height_dict[str(site)] = h_matr\n",
    "    \n",
    "# Becuase site 47 is exactly on the grid so it will lead to the error of divided by zero    \n",
    "lat_lon_dict['46'][5, 5, 46] = 0.001\n",
    "    \n",
    "with open('distance_of_all_sites.pk', 'wb') as file:\n",
    "    pk.dump(lat_lon_dict, file)\n",
    "\n",
    "with open('Height_of_all_sites.pk', 'wb') as file:\n",
    "    pk.dump(height_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_idw_value(p_matrix, x_list, y_list, xypair):\n",
    "    p = 2.0\n",
    "    # weight_matrix = np.zeros([len(xypair), p_matrix.shape[0], p_matrix.shape[1]])\n",
    "    weight_matrix = np.zeros([len(xypair), p_matrix.shape[1], p_matrix.shape[2]])\n",
    "    sum_weight_matrix = np.zeros([p_matrix.shape[1], p_matrix.shape[2]])\n",
    "    for m in range(p_matrix.shape[1]):\n",
    "        for n in range(p_matrix.shape[2]):\n",
    "            for i in range(len(xypair)):\n",
    "                if (m, n) in xypair:\n",
    "                    weight_matrix[i, m, n] = 1\n",
    "                    sum_weight_matrix[m, n] = 1\n",
    "                else:\n",
    "                    weight_matrix[i, m, n] = 1 / (math.sqrt((m - x_list[i])**2 + (n - y_list[i])**2) ** p)\n",
    "                    sum_weight_matrix[m, n] += 1 / (math.sqrt((m - x_list[i])**2 + (n - y_list[i])**2) ** p)\n",
    " \n",
    "    for z in range(p_matrix.shape[0]):\n",
    "        sum_matrix = np.zeros([p_matrix.shape[1], p_matrix.shape[2]])\n",
    "        for i in range(len(xypair)):\n",
    "            sum_matrix += weight_matrix[i] * p_matrix[z, x_list[i], y_list[i]]\n",
    "        for j in range(len(xypair)):\n",
    "            sum_matrix[x_list[j], y_list[j]] = p_matrix[z, x_list[j], y_list[j]]\n",
    "        p_matrix[z] = sum_matrix / sum_weight_matrix\n",
    "        \n",
    "    return p_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the paper, the elevation and the value of pm2.5 has the following relationship.\n",
    "\n",
    "$\\frac{1}{e^{\\frac{H - H_{st}}{H_{st}}}}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denominator of the IDW\n",
    "# It should contain all the distance of all the sites relative to the target grid and sum them all.\n",
    "denominator = np.empty((76, 11, 11))\n",
    "for site in range(76):\n",
    "    for i in range(11):\n",
    "        for j in range(11):\n",
    "            denominator[site, i, j] = ((1 / np.square(lat_lon_dict[str(site)][i , j])).sum())\n",
    "                            #* np.exp((height_dict[str(site)][i, j] - height_dict[str(site)][5, 5]) / height_dict[str(site)][5, 5]))\n",
    "            \n",
    "\n",
    "# Numerator of IDW\n",
    "# It should contain all the distance of all the sites relative to the target grid.\n",
    "# The difference between denominator and numerator is that numerator stores the individual data of the distances\n",
    "# to the target grid and denominator sums them all.\n",
    "numerator = np.empty((76, 11, 11, 76))\n",
    "for site in range(76):\n",
    "    for i in range(11):\n",
    "        for j in range(11):\n",
    "            numerator[site, i, j] = 1 / np.square(lat_lon_dict[str(site)][i , j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24096, 76, 11, 11)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "site_map = np.zeros((valid_test_set.loc[training_date].shape[0], 76, 11, 11))\n",
    "\n",
    "print(site_map.shape)\n",
    "\n",
    "# r stands for row\n",
    "for idx, pm25 in valid_test_set.loc[training_date, (slice(None), 'PM2.5')].iterrows():\n",
    "    all_pm25_list = pm25.values\n",
    "    for site in range(76):\n",
    "        for i in range(11):\n",
    "            for j in range(11):\n",
    "                a = (all_pm25_list * numerator[site, i, j]).sum()\n",
    "                site_map[r, site, i, j] = a / denominator[site, i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_cnn_site_map.pk', 'wb') as file:\n",
    "    pk.dump(site_map, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8760, 76, 11, 11)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "site_map = np.zeros((valid_test_set.loc[val_date].shape[0], 76, 11, 11))\n",
    "\n",
    "print(site_map.shape)\n",
    "\n",
    "# r stands for row\n",
    "for idx, pm25 in valid_test_set.loc[val_date, (slice(None), 'PM2.5')].iterrows():\n",
    "    all_pm25_list = pm25.values\n",
    "    for site in range(76):\n",
    "        for i in range(11):\n",
    "            for j in range(11):\n",
    "                a = (all_pm25_list * numerator[site, i, j]).sum()\n",
    "                site_map[r, site, i, j] = a / denominator[site, i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-260a8817d050>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val_cnn_site_map.pk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('val_cnn_site_map.pk', 'wb') as file:\n",
    "    pk.dump(site_map, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val_cnn_site_map.pk', 'rb') as file:\n",
    "    val_cnn_site_map = pk.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8760, 11, 11)\n",
      "(8760, 11, 11, 1)\n",
      "(8753, 11, 11, 1)\n",
      "(8748, 11, 11, 1)\n",
      "(8747, 11, 11, 1)\n"
     ]
    }
   ],
   "source": [
    "cnn_val = val_cnn_site_map[:, 0]\n",
    "print(cnn_val.shape)\n",
    "cnn_val = np.expand_dims(cnn_val, axis = 3)\n",
    "print(cnn_val.shape)\n",
    "cnn_val = np.delete(cnn_val, slice(0, 7), axis = 0)\n",
    "print(cnn_val.shape)\n",
    "cnn_val = np.delete(cnn_val, slice(-6, -1), axis = 0)\n",
    "print(cnn_val.shape)\n",
    "cnn_val = np.delete(cnn_val, -1, axis = 0)\n",
    "print(cnn_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_val = val_cnn_site_map[:, 0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8760, 1, 11, 11)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8747, 1, 11, 11)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_val[7:-6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((44,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
